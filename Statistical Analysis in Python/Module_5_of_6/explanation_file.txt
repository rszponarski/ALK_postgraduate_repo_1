SPIS TREŚCI:
1. Co to jest KLASTROWANIE?
2. Jaki jest cel klastrowania?
3. Jaki jest cel algorytmu K-means?
4. Co to ODLEGŁOŚĆ EUKLIDESOWA?
5. Jak działa algorytm k-średnich (K-Means)?
6. Przykład działania algorytmu K-means.
7. Jakie są ograniczenia algorytmu K-means?
8. Dobór liczby klastrów w K-means.
9. Czym jest random state? Jak go ustawić?
10. Jaka jest różnica między kmeans.fit(df_scaled){TASK1} a kmeans.fit_predict(df_scaled){TASK2}?
_____________________________________________

1. Co to jest KLASTROWANIE?
Grupowanie na klastry (klastrowanie) to technika eksploracyjnej analizy danych, która ma na celu grupowanie podobnych
obiektów w zbiory (nazywane klastrami).
**************************************************************************************
2. Jaki jest cel klastrowania?
---Znajdowanie podobieństw w danych:
Grupowanie pozwala zidentyfikować obiekty w danych, które są do siebie podobne na podstawie wybranych cech
(np. wartości w kolumnach).
Obiekty w jednym klastrze są bardziej podobne do siebie niż do obiektów w innych klastrach.
---Wykrywanie ukrytych struktur:
Czasami dane mają naturalne podziały lub struktury, które nie są oczywiste (np. grupy klientów o różnych preferencjach).
Klastrowanie pomaga je odkryć bez wcześniejszej wiedzy na temat tych grup.
---Segmentacja:
W różnych dziedzinach, takich jak marketing, biologia, czy analiza danych, klastrowanie pomaga podzielić dane na
segmenty w celu lepszego zrozumienia grup.
**************************************************************************************
3. Jaki jest cel algorytmu K-means?
Celem algorytmu K-means jest podzielenie zbioru danych na k podzbiorów (klastry), w taki sposób, aby punkty w każdym
klastrze były jak najbardziej do siebie podobne, a różne klastry były od siebie jak najbardziej różne. Kluczową miarą
jest odległość (najczęściej euklidesowa) między punktami i środkami klastrów.
**************************************************************************************
4. Co to ODLEGŁOŚĆ EUKLIDESOWA?
Odległość euklidesowa to miara odległości między dwoma punktami w przestrzeni, obliczana zgodnie z zasadami geometrii
euklidesowej. W praktyce jest to długość najkrótszego odcinka (prostej) łączącego te dwa punkty.
Dla dwóch punktów A(x1, y1), B(x2, y2):
	d(AB) = [ (x2-x1)^2 + (y2-y1)^2 ]^0.5
**************************************************************************************
5. Jak działa algorytm k-średnich (K-Means)?
--> Wybór liczby klastrów (k):
Na początku definiujemy liczbę klastrów, na które chcemy podzielić dane (np. n_clusters=3).
--> Inicjalizacja centroids (środków klastrów):
Algorytm losowo wybiera punkty początkowe, które reprezentują "środek" każdego klastra.
--> Przypisywanie punktów:
Każdy punkt danych jest przypisany do najbliższego centroidu na podstawie odległości (np. euklidesowej).
--> Aktualizacja centroidów:
Algorytm oblicza nowy "środek" każdego klastra jako średnią współrzędnych punktów przypisanych do tego klastra.
--> Iteracja:
Punkty są przypisywane do nowych centroidów, a centroidy są ponownie aktualizowane.
Proces powtarza się, aż zmiany przestaną być istotne (tzn. centroidy przestają się przesuwać).
**************************************************************************************
6. Przykład działania algorytmu K-means.
Wyobraź sobie dane 2D (punkty na płaszczyźnie):
Wybierasz k=2 (chcesz podzielić punkty na 2 grupy).
Algorytm losowo wybiera dwa punkty jako centroidy (np. C1 i C2).
Oblicza odległości każdego punktu do C1 i C2 Punkty najbliższe C1 przypisuje do klastra 1, a punkty najbliższe C2 do
klastra 2.
Oblicza nowe centroidy dla klastra 1 i klastra 2, biorąc średnie współrzędnych punktów w każdym klastrze.
Powtarza proces przypisywania i aktualizacji centroidów, aż centroidy przestaną się zmieniać.
**************************************************************************************
7. Jakie są ograniczenia algorytmu K-means?
  -> Wybór liczby klastrów (k):
Algorytm wymaga podania k, co w praktyce może być trudne do ustalenia bez wcześniejszej wiedzy o danych.
Można użyć metod takich jak metoda łokcia lub Silhouette Score, aby dobrać optymalne k.
  -> Kształt klastrów:
Algorytm najlepiej działa dla sferycznych, równomiernie rozłożonych klastrów. Dla klastrów o bardziej złożonych
kształtach może działać źle.
  -> Czułość na inicjalizację:
Wynik zależy od początkowych centroidów. Złe początkowe centroidy mogą prowadzić do suboptymalnego wyniku.
Można temu zaradzić, stosując technikę k-means++ do inteligentnej inicjalizacji centroidów.
  -> Odległość jako miara podobieństwa:
Jeśli dane mają różne jednostki lub różną skalę, algorytm może działać źle, dlatego standaryzacja danych
(np. przez StandardScaler) jest kluczowa.
**************************************************************************************
8. Dobór liczby klastrów w K-means.
Dobór liczby klastrów to kluczowy krok w algorytmie K-means. W praktyce istnieją różne metody, które pomagają znaleźć
optymalną wartość k:
---Metody doboru liczby klastrów:
   -> Metoda "łokcia" (Elbow Method):
Polega na obliczeniu sumy kwadratów odległości punktów od najbliższego środka klastra (inertia) dla różnych wartości k.
Następnie szuka się punktu, w którym zmniejszanie liczby klastrów przestaje znacząco poprawiać wyniki.
Jest to tzw. "łokieć" na wykresie.
  -> Średni współczynnik silhouette (Silhouette Score):
Mierzy, jak dobrze punkty są przypisane do swoich klastrów w porównaniu z innymi klastrami.
Wartość wynosi od -1 (złe klastry) do 1 (idealne klastry). Najlepsze k to takie, które maksymalizuje tę wartość.
  -> Metoda "spadku gradientu" (Gap Statistic):
Porównuje różnicę między jakością klastrów w danych rzeczywistych a danych losowych.
Wybiera k, które najlepiej odróżnia dane rzeczywiste od losowych.
  -> Eksploracja wizualna:
Dla danych 2D lub 3D można wizualizować dane i ręcznie ocenić naturalne grupy.
  -> Specyfika problemu:
Liczba klastrów może być narzucona przez domenę problemu (np. klasyfikacja biologiczna, segmentacja klientów itp.).
**************************************************************************************
9. Czym jest random state? Jak go ustawić?
---Czym jest random_state?
 * Parametr random_state kontroluje losowość inicjalizacji centrów klastrów w algorytmie K-means.
 * Algorytm K-means rozpoczyna od losowego wyboru początkowych centrów klastrów. Wynik może się różnić w zależności
 od tej inicjalizacji.
 * random_state jest jak "nasiono" dla generatora liczb losowych. Dzięki temu, ustawienie konkretnej wartości powoduje,
  że wyniki będą identyczne przy każdym uruchomieniu algorytmu na tych samych danych.

---Dlaczego warto ustawiać random_state?
 * Reproducowalność:
W badaniach lub produkcji ważne jest, aby inne osoby mogły odtworzyć Twoje wyniki.
 * Porównywalność:
Ustawienie tej samej wartości random_state pozwala porównywać wyniki różnych parametrów k lub innych modyfikacji
algorytmu.
 * Jak wybrać wartość random_state?
Wartość jest arbitralna (np. 42, 0, 1234). Najczęściej używa się "symbolicznego" 42,
nawiązującego do „Autostopem przez Galaktykę” Douglasa Adamsa.
Ważne jest, aby zawsze dokumentować, jaką wartość wybrałeś, aby zachować reprodukowalność.

---Wzajemna zależność liczby klastrów i random_state
Liczba klastrów k nie zależy bezpośrednio od random_state.
Użycie tego samego random_state zapewnia, że wyniki dla danego k są powtarzalne. Możesz więc skupić się na analizie
efektów zmiany liczby klastrów bez wpływu losowości.

"Ustawienie wartości random_state jako 42 (lub jakąkolwiek inną stałą wartość) nie wpływa bezpośrednio na działanie
samego algorytmu K-means, ale ma na celu kontrolowanie losowości w procesie inicjalizacji centrów klastrów.
42 to nawiązanie do książki "Autostopem przez Galaktykę" Douglasa Adamsa, gdzie 42 jest "odpowiedzią na ostateczne
pytanie o życie, wszechświat i wszystko inne""
**************************************************************************************
10. Jaka jest różnica między kmeans.fit(df_scaled) a kmeans.fit_predict(df_scaled)?
Cechą wspólną T1 i T2 jest standaryzacja danych:
	scaler = StandardScaler()
	df_scaled = scaler.fit_transform(df)
Ale w T1:
	kmeans.fit(df_scaled)
Natomiast w T2:
	kmeans.fit_predict(df_scaled)

Różnica między kmeans.fit(df_scaled) a kmeans.fit_predict(df_scaled) leży w tym, co zwracają i co robią te metody. Przyjrzyjmy się szczegółowo:

---------A. kmeans.fit(df_scaled)
**Co robi?
Dopasowuje model K-means do danych.
Ustala położenie centroidów dla podanej liczby klastrów.
Nie zwraca niczego istotnego dla dalszego użycia (zwraca tylko sam obiekt KMeans).

**Jak działa w praktyce?
Oblicza centroidy klastrów.
Model K-means jest gotowy do użycia np. do przewidywania klastrów za pomocą metody predict().

---------B. kmeans.fit_predict(df_scaled)
**Co robi?
Dopasowuje model K-means do danych (tak jak fit()).
Dodatkowo zwraca etykiety klastrów (czyli informację o tym, do którego klastra należy każdy punkt w danych).

**Jak działa w praktyce?
Oblicza centroidy klastrów.
Od razu przypisuje punkty do klastrów.
Zwraca etykiety (numery klastrów) dla każdego punktu.

**************************************************************************************
11. Co to algorytm t-SNE?
--> t-SNE (t-Distributed Stochastic Neighbor Embedding)
-to zaawansowana metoda redukcji wymiarowości, która jest szeroko stosowana w analizie danych o wysokiej liczbie wymiarów.

Celem t-SNE jest przedstawienie danych w sposób dwuwymiarowy lub wielowymiarowy, jednocześnie utrzymując ich strukturę podobieństwa w oryginalnej przestrzeni.

*** Główne cechy t-SNE:
==> Redukcja wymiarowości: t-SNE zmniejsza liczbę cech z wysokiej liczby (np. 100 lub więcej) do dwóch lub trzech wymiarów, co pozwala na lepszą wizualizację danych.
==> Uwzględnianie odległości podobieństwa: Algorytm stara się zachować jak najwięcej podobieństw między danymi. Zamiast zwykłych odległości euklidesowych, t-SNE koncentruje się na tzw. odległościach prawdopodobieństw, które bardziej odzwierciedlają stosunek podobieństwa niż zwykłe wartości odległości. !!!!!
==> Nieliniowa transformacja: W przeciwieństwie do innych metod, takich jak PCA (Principal Component Analysis), t-SNE często stosuje nieliniową transformację danych, aby lepiej oddać bardziej skomplikowane zależności między punktami danych.

*** Proces działania t-SNE:
---> Obliczanie podobieństwa: Na początku obliczana jest odległość prawdopodobieństwa między danymi (np. Euclidean Distance) lub inne podobieństwa dla większych zestawów danych.
	tzn. Na początku algorytm oblicza odległości euklidesowe pomiędzy punktami w oryginalnej przestrzeni wielowymiarowej. Następnie te odległości są przekształcane w miary podobieństwa za pomocą funkcji Gaussa (rozmytego rozkładu prawdopodobieństwa= soft probability distribution).
---> Stworzenie rozkładu odległości: Na podstawie podobieństw tworzy się macierz prawdopodobieństw dla najbliższych sąsiadów (tzw. "soft probability distribution").
---> Obniżenie wymiarów: Następuje nieliniowa redukcja wymiarów do wybranej liczby komponentów (najczęściej 2 lub 3), starając się jak najlepiej odzwierciedlić stosunki podobieństwa w wyższej przestrzeni.

$$$ Koszty obliczeniowe:
T-SNE jest kosztowną metodą pod względem obliczeń, ponieważ każda iteracja wykorzystuje algorytm iteracyjny, który znajduje optymalne położenie punktów w przestrzeni niskowymiarowej.
Porównanie t-SNE z innymi metodami:
PCA (Principal Component Analysis) – PCA oblicza liniowe transformacje, które maksymalizują wariancję danych, ale niekoniecznie uwzględnia nieliniowe zależności.

t-SNE – Stosuje nieliniową transformację, aby lepiej odwzorować podobieństwa między danymi w wysokiej liczbie wymiarów.

***Zalety t-SNE:
-Zachowuje strukturę podobieństw w danych o wysokich wymiarach.
-Umożliwia wizualizację bardziej skomplikowanych zależności między danymi.
-Bardziej odpowiednia dla danych z nieliniowymi cechami.

***Wady t-SNE:
-Bardzo czasochłonna dla dużych zbiorów danych.
-Wyniki mogą być podatne na różne konfiguracje hiperparametrów (np. liczba iteracji, rozmiar próbki sąsiadów).
-Nieliniowa transformacja może prowadzić do rozbieżności w niektórych przypadkach.
**************************************************************************************
12. Dlaczego używa się "rozmytego rozkładu" w t-SNE?
--Unikanie ostrych decyzji:

Zamiast przypisywać punkty do jednego sąsiada, algorytm bierze pod uwagę wpływ wszystkich sąsiadów w zależności od ich odległości (dzięki funkcji Gaussa). W ten sposób uzyskuje bardziej realistyczny obraz podobieństw.

Lokalne podobieństwa:
Rozkład "rozmyty" pozwala algorytmowi lepiej skupić się na lokalnych relacjach, ponieważ punkty bliskie będą miały większe wartości, a odległe - mniejsze.
Podsumowanie:
"Rozmyty rozkład prawdopodobieństwa" i "soft probability distribution" to w zasadzie to samo, wyrażone w różnych językach.
Oznaczają one sytuację, w której prawdopodobieństwa są przypisywane w sposób ciągły (miękki), zamiast być binarne.
W t-SNE wykorzystuje się tę koncepcję, aby zbudować realistyczne relacje między punktami w przestrzeni wysokowymiarowej i lepiej odwzorować je w przestrzeni niskowymiarowej.

